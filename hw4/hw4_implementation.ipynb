{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f53c4ec-6110-4cd0-8cb8-53cf92e8dd80",
   "metadata": {},
   "source": [
    "### CS 268 - HW4\n",
    "* Name: Berkay Guler\n",
    "* Department: NetSys "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aaf138-dae9-414d-8ec6-1f75405f78df",
   "metadata": {},
   "source": [
    "# Problem 1)\n",
    "### A - Augmented Lagrangian:\n",
    "This method uses a dynamic penalty term that is updated at each iteration to solve for optimization problems with equality constraints only. Given the optimization problem\n",
    "$$ \\min_{x} {f(x)}$$\n",
    "$$ \\text{subject to } h(x) = 0$$\n",
    "\n",
    "Augmented Lagrangian method employs the penalty function\n",
    "$$ p(x) = \\frac{1}{2} \\rho \\sum_{i}{(h_i{(x)})^2} - \\sum_{i}{\\lambda_i h_i(x)}$$\n",
    "where at each iteration $ \\lambda $ is updated as\n",
    "$$ \\lambda^{(k+1)} = \\lambda^{(k)} - \\rho^{(k)} h(x)$$\n",
    "and $ \\rho $ is updated as\n",
    "$$ \\rho^{(k+1)} = \\rho^{(k)} \\gamma$$\n",
    "\n",
    "Minimizing the unconstrained problem $ f(x) + p(x) $ repeteadly with updated $p(x)$ penalty function until convergence is an approximation to minimizing $ f(x) $ in the constrained fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3883d032-5a9c-4d31-8ebe-d9c7199f3196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "augmented_lagrange_method (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "function augmented_lagrange_method(f, h, x, k_max; ρ=1, γ=2)\n",
    "    #= Minimizes f subject to h(x) = 0\n",
    "    source code taken (modified) from: K&W Algorithms for Optimization page 183\n",
    "    returns:\n",
    "        x: point where min occurs\n",
    "        descents: number of gradient descents performed\n",
    "        convergence_measure: The absolute difference between the last obtained\n",
    "            minimum value of the objective function f and the one before the last.\n",
    "    =#\n",
    "    λ = zeros(length(h(x)))\n",
    "    descents = 0\n",
    "    old_f_value = f(x)\n",
    "    convergence_measure = 0\n",
    "    for k in 1:k_max\n",
    "        # penalty function\n",
    "        p = x -> ρ/2*sum(h(x).^2) - λ⋅h(x)\n",
    "        # function to optimize\n",
    "        objective = x -> (f(x) + p(x))\n",
    "        # steepest gradient descent\n",
    "        x, descent = gradient_descent(objective, x)\n",
    "        new_f_value = f(x)\n",
    "        convergence_measure = abs(new_f_value - old_f_value)\n",
    "        λ = λ .- ρ*h(x)\n",
    "        ρ *= γ\n",
    "        descents += descent\n",
    "        old_f_value = new_f_value\n",
    "    end\n",
    "    return x, descents, convergence_measure\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed95122-efb1-44c7-9656-5d258ec6d08b",
   "metadata": {},
   "source": [
    "### B - Minimization Method:\n",
    "I am using the steepest descent algorithm with exact line search to solve for the minimization problem occuring in the Augmented Lagrangian Loop. We modify the penalty function at each iteration, leading to gradients of the objective function changing at each iteration. Therefore, I am calculating the gradient of the objective function using numerical differentiation methods built into Julia. One could also exploit the fact that $f(x)$ is fixed. Hence, analythically calculating the gradient of $f(x)$ saves computation. However, $p(x)$ is not fixed and its gradient cannot be calculated with just one hard-coded function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fea31d10-c61e-4222-8e76-6fbf56795a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bracket_minimum (generic function with 2 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ForwardDiff\n",
    "\n",
    "function gradient_descent(f, x, k_max=100000, α=0.0000001)\n",
    "    #= performs steepest descent until convergence\n",
    "    returns:\n",
    "        x_current: point where min occurs\n",
    "        k: number of descents taken\n",
    "    =#\n",
    "    x_current = x\n",
    "    k = 1\n",
    "    while k < k_max\n",
    "        ∇_x_current = ForwardDiff.gradient(f, x_current)\n",
    "        d = -∇_x_current / sqrt(dot(∇_x_current, ∇_x_current))\n",
    "        α = line_search(f, x_current, d)\n",
    "        x_current = x_current + α*d\n",
    "        if norm(∇_x_current) < 0.01\n",
    "            break\n",
    "        end\n",
    "        k += 1\n",
    "    end\n",
    "    return x_current, k\n",
    "end\n",
    "\n",
    "\n",
    "function line_search(f, x, d)\n",
    "    # source code taken (but modified) from: K&W Algorithms for Optimization page 54\n",
    "    # returns optimal step size by performing line search of f along d and starting at x\n",
    "    objective = α -> f(x + α*d)\n",
    "    # 1-D minimization solver \n",
    "    α = minimize(objective)\n",
    "    return α\n",
    "end\n",
    "\n",
    "\n",
    "function minimize(\n",
    "        func, initial_point=0, \n",
    "        initial_step_size=1e-2, convergence_thr=0.000000001;)\n",
    "    # source code taken (but modified) from: K&W Algorithms for Optimization page 41\n",
    "    # returns the value at which func attains its min value\n",
    "    # optimization method used = Golden Section Search \n",
    "    # source(s) of code used = K&W Algorithms for Optimization, page 41\n",
    "    \n",
    "    a, b = bracket_minimum(func, initial_point, s=initial_step_size, k=2.0)\n",
    "    ρ = φ - 1\n",
    "    d = ρ * b  + (1 - ρ)*a\n",
    "    yd = func(d)\n",
    "    x_estimate_curr = (a + b)/2\n",
    "    x_estimate_prev = x_estimate_curr + convergence_thr + 0.0001  \n",
    "    eval_count = 1\n",
    "    \n",
    "    while abs(x_estimate_curr - x_estimate_prev) > convergence_thr\n",
    "        c = ρ*a + (1 - ρ)*b \n",
    "        yc = func(c)\n",
    "        if yc < yd\n",
    "            b, d, yd = d, c, yc\n",
    "        else\n",
    "            a, b = b, c\n",
    "        end\n",
    "        x_estimate_prev = x_estimate_curr\n",
    "        x_estimate_curr = (a + b)/2\n",
    "        eval_count += 1\n",
    "    end\n",
    "    # return x_estimate_curr, abs(x_estimate_curr - x_estimate_prev), eval_count\n",
    "    return x_estimate_curr\n",
    "end\n",
    "\n",
    "\n",
    "function bracket_minimum(fnc, x=0; s=1e-4, k=2.0)\n",
    "    # returns an interval in which fnc has a minimum\n",
    "    # source(s) of code used = K&W Algorithms for Optimization, page 36\n",
    "    a, ya = x, fnc(x)\n",
    "    b, yb = a + s, fnc(a + s) \n",
    "    if yb > ya\n",
    "        a, b = b, a\n",
    "        ya, yb = yb, ya\n",
    "        s = -s\n",
    "    end\n",
    "    while true\n",
    "        c, yc = b + s, fnc(b + s) \n",
    "        if yc > yb\n",
    "            return a < c ? (a, c) : (c, a)\n",
    "        end\n",
    "        a, ya, b, yb = b, yb, c, yc\n",
    "        s *= k\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627cbf76-642f-40e0-a9af-7df8862444f7",
   "metadata": {},
   "source": [
    "### C - Test Functions:\n",
    "I am using the test functions taken from my HW2 submission. I am using the same constraint of restricting the sum of squares of all entries to be equal to some non-negative constant $k$.\n",
    "\n",
    "1) 10-D Booth's Function:\n",
    "$$ f(x) = \\sum_{i=1}^{9} {[(x_i + 2x_{i+1}-7)^2 + (2x_i + x_{i+1}-5)^2]}$$\n",
    "\n",
    "2) 10-D Rosenbrock Function:\n",
    "$$ f(x) = \\sum_{i=1}^{9} {[(a-x_i)^2+b(x_{i+1}-x_i^2)^2]}$$\n",
    "\n",
    "Hence the final optimization objective looks like\n",
    "$$ \\min_{x} {f(x)}$$\n",
    "$$ \\text{subject to } h(x) = 0$$\n",
    "$$ \\text{where } h(x) = (\\sum_{i} {x_i^2}) - k$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f3fbc62-321c-46f5-9e2e-79a096aafcad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_exp_uniform_ndim (generic function with 4 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Base.MathConstants\n",
    "\n",
    "# 10-D Rosenbrock Test Function\n",
    "function rosenbrock_10D(x, a=1, b=1)\n",
    "    summation = 0\n",
    "    for i in 1:9\n",
    "        summation += ((a - x[i])^2 + b*(x[i+1]-x[i]^2)^2)\n",
    "    end\n",
    "    return summation\n",
    "end\n",
    "\n",
    "# quadractic function\n",
    "function booths_10D(x)\n",
    "    summation = 0\n",
    "    for i in 1:9\n",
    "        summation += ((x[i] + 2*x[i+1] - 7)^2 + (2*x[i] + x[i+1] - 5)^2)\n",
    "    end\n",
    "    return summation\n",
    "end\n",
    "\n",
    "function constraint(x, k=5)\n",
    "    # returns the value of constraint h\n",
    "    summation = 0\n",
    "    for i in 1:length(x)\n",
    "        summation += (x[i])^2\n",
    "    end\n",
    "    return summation - k\n",
    "end\n",
    "\n",
    "function sample_exp_uniform_ndim(num_samples=100, ndim=10, range=7)\n",
    "    # returns 10-D test points\n",
    "    columns = []\n",
    "    for i in 1:ndim\n",
    "        # sample num_samples numbers from [-range, range]\n",
    "        x = range*(rand(num_samples)) .* rand((-1, 1), num_samples)\n",
    "        # raise to the power of e and select a random sign\n",
    "        y = e.^x .* rand((-1, 1), 100)\n",
    "        append!(columns, [y])\n",
    "    end\n",
    "    return hcat(columns...)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe16f90-62bc-408e-ac1a-4e9ba7b9b173",
   "metadata": {},
   "source": [
    "### D - Results:\n",
    "The following metrics are used to evaluate the performance on both test functions. For each metric, mean and standard deviation over 100 test points are reported. Each test point $x^{(i)}=(x_1, x_2, ..., x_{10})$ is selected as follows.\n",
    "$$Y_i∼Uniform(-a,a)$$ $$X^{(i)} = se^{Y_i}$$ $$ \\text{ where } s∼Uniform(\\{-1, 1\\})$$\n",
    "\n",
    "* **Absolute error in constraint:** The distance of $h(x)$ from 0. Measures how well the solution satisfies the constraint.\n",
    "* **Elapsed Time:** Elapsed time in miliseconds.\n",
    "* **Last Value of Convergence Measure:** The absolute difference between the last obtained minimum value of the objective function $f$ and the one before the last.\n",
    "* **Number of Descents:** Sum of number of total descents performed within the gradient descent loops.\n",
    "\n",
    "**Note:** I do not report the absolute error in $f$ because I could not calculate the analytical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1827c3a-9e41-4b66-a541-e9f98ac2a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max iteration for augmented lagrangian\n",
    "max_iteration = 20\n",
    "function_names = [\"Booth's\", \"Rosenbrock\"] \n",
    "functions = [booths_10D, rosenbrock_10D]  \n",
    "# store results in a dict\n",
    "database = Dict()\n",
    "\n",
    "test_points = sample_exp_uniform_ndim()\n",
    "\n",
    "for (function_, f) in zip(function_names, functions)\n",
    "    if !(function_ in keys(database))\n",
    "        database[function_] = Dict(\"error\" => [], \"time\" => [], \"conv\" => [], \"descent\" => [])\n",
    "    end\n",
    "        \n",
    "    for i in 1:size(test_points)[1]\n",
    "        test_point = test_points[i, :]\n",
    "        @assert length(test_point) == 10\n",
    "        \n",
    "        tic = time()\n",
    "        x, num_descents, convergence_measure = augmented_lagrange_method(\n",
    "            f, constraint, test_point, max_iteration)\n",
    "        # 1000 to convert to ms\n",
    "        elapsed_time = (time() - tic) * 1000\n",
    "\n",
    "        push!(database[function_][\"error\"], abs(constraint(x)))\n",
    "        push!(database[function_][\"time\"], elapsed_time)\n",
    "        push!(database[function_][\"descent\"], num_descents)\n",
    "        push!(database[function_][\"conv\"], convergence_measure)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9edd974a-e298-493c-b27b-23bb5ac4d2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 1.0285860192888929e-7\n",
       " 1.7259725429359137e-9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Statistics \n",
    "\n",
    "# get means for each columns\n",
    "mean_error_col = [mean(database[f_name][\"error\"]) for f_name in keys(database)]\n",
    "mean_time_col = [mean(database[f_name][\"time\"]) for f_name in keys(database)]\n",
    "mean_descent_col = [mean(database[f_name][\"descent\"]) for f_name in keys(database)]\n",
    "mean_conv_col = [mean(database[f_name][\"conv\"]) for f_name in keys(database)]\n",
    "\n",
    "# get stds for each columns\n",
    "std_error_col = [std(database[f_name][\"error\"]) for f_name in keys(database)]\n",
    "std_time_col = [std(database[f_name][\"time\"]) for f_name in keys(database)]\n",
    "std_descent_col = [std(database[f_name][\"descent\"]) for f_name in keys(database)]\n",
    "std_conv_col = [std(database[f_name][\"conv\"]) for f_name in keys(database)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071a5547-7d0f-4376-a574-61a68bd79e07",
   "metadata": {},
   "source": [
    "### D.1 - Mean Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7755d69e-4a07-4e23-8a2d-949686e7219c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table with 5 columns and 2 rows:\n",
       "     Func                   Error       Convergence  NumDescents  Time\n",
       "   ┌─────────────────────────────────────────────────────────────────────\n",
       " 1 │ 10-D Booth's Function  2.61456e-9  1.25344e-7   68.06        8.1879\n",
       " 2 │ 10-D Rosenbrock        2.29206e-9  1.43021e-9   75.92        2.93625"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using TypedTables\n",
    "\n",
    "mean_table = Table(\n",
    "    Func = [\"10-D Booth's Function\", \"10-D Rosenbrock\"], \n",
    "    Error = mean_error_col,\n",
    "    Convergence = mean_conv_col, NumDescents = mean_descent_col,\n",
    "    Time = mean_time_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68802b6-ed3f-4cc8-b72d-bb75ff267351",
   "metadata": {},
   "source": [
    "### D.1 - Std Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58349864-41e1-4232-b4de-ab1effc8075b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table with 5 columns and 2 rows:\n",
       "     Func                   Error       Convergence  NumDescents  Time\n",
       "   ┌─────────────────────────────────────────────────────────────────────\n",
       " 1 │ 10-D Booth's Function  1.1748e-9   1.02859e-7   1.72808      75.3007\n",
       " 2 │ 10-D Rosenbrock        1.42746e-9  1.72597e-9   5.29852      23.4223"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_table = Table(\n",
    "    Func = [\"10-D Booth's Function\", \"10-D Rosenbrock\"], \n",
    "    Error = std_error_col,\n",
    "    Convergence = std_conv_col, NumDescents = std_descent_col,\n",
    "    Time = std_time_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cdda3f-74dc-4c63-99a6-66bd3f823cbd",
   "metadata": {},
   "source": [
    "# Problem 2)\n",
    "### A - Simulated Annealing:\n",
    "Simulated annealing method samples a datapoint from a probability distribution conditioned on the current point. The sampled point is always accepted if it leads to a descent. If the sampled point leads to an ascent the new point is accepted with some probability which is a function of the temperature. \n",
    "\n",
    "To obtain the new datapoint $\\tilde{x}$, I am using a Gaussian distribution with mean $x$, the current point, and with unit diagonal covariance matrix.\n",
    "$$ P(\\tilde{x}|x)= N(\\tilde{x};x,\\sigma^2 I)$$\n",
    "Another way to view this is adding zero mean Gaussian noise to the current point $x$ to obtain $\\tilde{x}$\n",
    "$$ \\tilde{x} = x + \\sigma \\epsilon \\text{  where } \\epsilon \\sim N(0, I)$$ \n",
    "The obtained point is discarded or accepted based on the following decision rule. \n",
    "$$\n",
    "\\begin{cases} \n",
    "\\text{always accept } & \\text{if } \\Delta y \\le 0 \\\\\n",
    "\\text{accept with probability }e^{-\\Delta y/t} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $ \\Delta y = f(\\tilde{x}) - f(x$) and $t$ follows the fast annealing schedule. \n",
    "$$ t^{(k+1)} = t^{(k)} \\frac{k}{k+1} $$ \n",
    "\n",
    "In practice, initial temperature, decay schedule, and variance of the noise greatly affects the performance of the algorithm. I tested several different parameters to minimize the absolute error.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4c4b449-40c7-4107-8331-ffa40fb2b40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simulated_annealing (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Random\n",
    "\n",
    "function fast_annealing_scheduler(t, k)\n",
    "    # returns the next temperature given the current temperature\n",
    "    return t * (k/(k+1))\n",
    "end\n",
    "\n",
    "\n",
    "function simulated_annealing(f, x, t_0, scheduler, k_max; verbose=false, report_every_k=1000, σ=1)\n",
    "    #= performs fast simulated annealing with gaussian noise\n",
    "    returns:\n",
    "        x_best: the best point we come across during the search \n",
    "        num_ascents: number of ascents performed\n",
    "        num_descents: number of descents performed\n",
    "    =#\n",
    "    y = f(x)\n",
    "    x_best, y_best = x, y\n",
    "    m = length(x)\n",
    "    t = t_0\n",
    "    num_descents = 0\n",
    "    num_ascents = 0 \n",
    "    for k in 1 : k_max\n",
    "        x′ = x + σ * randn(m)\n",
    "        y′ = f(x′)\n",
    "        Δy = y′ - y\n",
    "        if Δy ≤ 0\n",
    "            x, y = x′, y′\n",
    "            num_descents += 1\n",
    "        elseif rand() < exp(-Δy/t)\n",
    "            x, y = x′, y′\n",
    "            num_ascents += 1\n",
    "        end\n",
    "        \n",
    "        if y′ < y_best\n",
    "            x_best, y_best = x′, y′\n",
    "        end\n",
    "        \n",
    "        if verbose && k % report_every_k == 0\n",
    "            println(\"\\nStep $k\")\n",
    "            println(x_best)\n",
    "            println(\"Temp: $t, prob: $(exp(-Δy/t))\\n\")\n",
    "        end\n",
    "        t = scheduler(t, k)\n",
    "    end\n",
    "    return x_best, num_ascents, num_descents\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8bcc67-ab03-4ce5-9b07-884b4ebaa0a4",
   "metadata": {},
   "source": [
    "### B - Test Functions:\n",
    "I am using the test functions taken from my HW2 submission. \n",
    "\n",
    "1) 10-D Booth's Function:\n",
    "$$ f(x) = \\sum_{i=1}^{9} {[(x_i + 2x_{i+1}-7)^2 + (2x_i + x_{i+1}-5)^2]}$$\n",
    "\n",
    "2) 10-D Rosenbrock Function:\n",
    "$$ f(x) = \\sum_{i=1}^{9} {[(a-x_i)^2+b(x_{i+1}-x_i^2)^2]}$$\n",
    "\n",
    "**Note 1:** For $a=1$, Rosenbrock function takes its minimum value\n",
    "$f(x^*) = 0$ at $x ^* = \\{x_i = 1| i=1,2,...,10\\}$\n",
    "\n",
    "**Note 2:** Because the minimum of Booth's function is not easy to obtain analythically, I am using Julia optim library to report an approximate absolute error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c2be5c-6372-4f57-80f6-5137ab07e42b",
   "metadata": {},
   "source": [
    "### C - Results:\n",
    "The following metrics are used to evaluate the performance on both test functions. For each metric, mean and standard deviation over 100 test points are reported. Each test point $x^{(i)}=(x_1, x_2, ..., x_{10})$ is selected as follows.\n",
    "$$Y_i∼Uniform(-a,a)$$ $$X^{(i)} = se^{Y_i}$$ $$ \\text{ where } s∼Uniform(\\{-1, 1\\})$$\n",
    "* **Absolute error:** Distance between the ground truth minimum value of function $f$ and the minimum found by the algorithm.\n",
    "* **Elapsed Time:** Elapsed time in miliseconds.\n",
    "* **Number of Ascents:** Number of total ascents performed. Ascending generally occurs at higher temperatures.\n",
    "* **Number of Descents:** Number of total descents performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b33005d-4e81-4722-86bb-a1e4010b991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Optim\n",
    "\n",
    "function_names = [\"Booth's\", \"Rosenbrock\"] \n",
    "functions = [booths_10D, rosenbrock_10D]\n",
    "database = Dict()\n",
    "# sample test pisoints\n",
    "test_points = sample_exp_uniform_ndim()\n",
    "# ground truth min values\n",
    "bt_res = optimize(booths_10D, test_points[1,:], LBFGS())\n",
    "bt_gt_min_x = Optim.minimizer(bt_res)\n",
    "bt_gt_min_f = booths_10D(bt_gt_min_x)\n",
    "rb_gt_min_x = [1 for i in 1:10]\n",
    "rb_gt_min_f = rosenbrock_10D(rb_gt_min_x)\n",
    "\n",
    "zip_iter = zip(function_names, functions, (bt_gt_min_f, rb_gt_min_f))\n",
    "\n",
    "# simulated annealing parameters\n",
    "initial_temp = 1000\n",
    "total_steps = 10000000\n",
    "report_freq = 100000\n",
    "sigma = 0.1\n",
    "verbose = false\n",
    "\n",
    "for (function_, f, min_f) in zip_iter\n",
    "    if !(function_ in keys(database))\n",
    "        database[function_] = Dict(\"error\" => [], \"time\" => [], \"ascent\" => [], \"descent\" => [])\n",
    "    end\n",
    "        \n",
    "    for i in 1:size(test_points)[1]\n",
    "        test_point = test_points[i, :]\n",
    "        @assert length(test_point) == 10\n",
    "        \n",
    "        tic = time()\n",
    "        x, num_asc, num_desc = simulated_annealing(\n",
    "            f, test_point, initial_temp,\n",
    "            fast_annealing_scheduler, total_steps,\n",
    "            report_every_k=report_freq,\n",
    "            verbose=verbose, σ=sigma)\n",
    "        elapsed_time = (time() - tic) * 1000\n",
    "        abs_error = abs(min_f - f(x))\n",
    "        \n",
    "        push!(database[function_][\"error\"], abs_error)\n",
    "        push!(database[function_][\"time\"], elapsed_time)\n",
    "        push!(database[function_][\"descent\"], num_desc)\n",
    "        push!(database[function_][\"ascent\"], num_asc)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "054f39f2-4ad4-4357-8d79-fa31f21c107d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 345.0588215116813\n",
       " 637.9315421142323"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_error_col = [mean(database[f_name][\"error\"]) for f_name in keys(database)]\n",
    "mean_time_col = [mean(database[f_name][\"time\"]) for f_name in keys(database)]\n",
    "mean_descent_col = [mean(database[f_name][\"descent\"]) for f_name in keys(database)]\n",
    "mean_ascent_col = [mean(database[f_name][\"ascent\"]) for f_name in keys(database)]\n",
    "\n",
    "std_error_col = [std(database[f_name][\"error\"]) for f_name in keys(database)]\n",
    "std_time_col = [std(database[f_name][\"time\"]) for f_name in keys(database)]\n",
    "std_descent_col = [std(database[f_name][\"descent\"]) for f_name in keys(database)]\n",
    "std_ascent_col = [std(database[f_name][\"ascent\"]) for f_name in keys(database)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0be868-3868-46c6-9f43-805ad35a0a88",
   "metadata": {},
   "source": [
    "### C.1 - Mean Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75e0abfe-e09f-42d6-8127-bc9b187c08dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table with 5 columns and 2 rows:\n",
       "     Func                   Error       NumAscents  NumDescents  Time\n",
       "   ┌────────────────────────────────────────────────────────────────────\n",
       " 1 │ 10-D Booth's Function  0.0136245   329.84      8296.69      1377.92\n",
       " 2 │ 10-D Rosenbrock        0.00850584  753.71      8456.42      1195.96"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_table = Table(\n",
    "    Func = [\"10-D Booth's Function\", \"10-D Rosenbrock\"], \n",
    "    Error = mean_error_col,\n",
    "    NumAscents = mean_ascent_col, NumDescents = mean_descent_col,\n",
    "    Time = mean_time_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfea3e5-90fc-438e-9392-67ac7adff41a",
   "metadata": {},
   "source": [
    "### C.1 - Std Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a5c4956-0ca3-4fd2-9bfc-41847d648cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table with 5 columns and 2 rows:\n",
       "     Func                   Error       Convergence  NumDescents  Time\n",
       "   ┌─────────────────────────────────────────────────────────────────────\n",
       " 1 │ 10-D Booth's Function  0.00279292  1.02859e-7   5215.73      47.4244\n",
       " 2 │ 10-D Rosenbrock        0.00187578  1.72597e-9   5045.88      38.1558"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_table = Table(\n",
    "    Func = [\"10-D Booth's Function\", \"10-D Rosenbrock\"], \n",
    "    Error = std_error_col,\n",
    "    Convergence = std_conv_col, NumDescents = std_descent_col,\n",
    "    Time = std_time_col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
